{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SocialProphet - Phase 2: Time Series Forecasting\n",
    "\n",
    "This notebook implements and evaluates the forecasting pipeline:\n",
    "1. Load preprocessed data\n",
    "2. Stationarity analysis\n",
    "3. Train individual models (Prophet, SARIMA, LSTM)\n",
    "4. Ensemble predictions\n",
    "5. Evaluate metrics\n",
    "6. Visualizations\n",
    "\n",
    "**Target Metrics:**\n",
    "- MAPE < 15%\n",
    "- RMSE < 15% of mean\n",
    "- R² > 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project imports\n",
    "from src.utils.config import Config\n",
    "from src.forecasting.stationarity import StationarityAnalyzer\n",
    "from src.forecasting.prophet_model import ProphetForecaster\n",
    "from src.forecasting.sarima_model import SARIMAForecaster\n",
    "from src.forecasting.lstm_model import LSTMForecaster\n",
    "from src.forecasting.ensemble import EnsembleForecaster\n",
    "from src.evaluation.metrics import ForecastMetrics\n",
    "from src.evaluation.visualizer import Visualizer\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "train_df = pd.read_csv(Config.PROCESSED_DATA_DIR / \"train_data.csv\", parse_dates=['ds'])\n",
    "test_df = pd.read_csv(Config.PROCESSED_DATA_DIR / \"test_data.csv\", parse_dates=['ds'])\n",
    "\n",
    "# Load Prophet-format data\n",
    "train_prophet = pd.read_csv(Config.PROCESSED_DATA_DIR / \"train_prophet.csv\", parse_dates=['ds'])\n",
    "test_prophet = pd.read_csv(Config.PROCESSED_DATA_DIR / \"test_prophet.csv\", parse_dates=['ds'])\n",
    "\n",
    "print(f\"Training data: {len(train_df)} rows, {len(train_df.columns)} columns\")\n",
    "print(f\"Test data: {len(test_df)} rows\")\n",
    "print(f\"\\nDate range: {train_df['ds'].min()} to {test_df['ds'].max()}\")\n",
    "print(f\"\\nTarget variable (y):\")\n",
    "print(f\"  Log scale range: [{train_df['y'].min():.2f}, {train_df['y'].max():.2f}]\")\n",
    "print(f\"  Original scale range: [{train_df['y_raw'].min():,.0f}, {train_df['y_raw'].max():,.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns for LSTM\n",
    "feature_cols = [col for col in train_df.columns if col not in ['ds', 'y', 'y_raw']]\n",
    "print(f\"Available features ({len(feature_cols)}): {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stationarity Analysis\n",
    "\n",
    "Before fitting SARIMA, we verify time series stationarity using:\n",
    "- **ADF Test**: Null hypothesis = non-stationary (reject if p < 0.05)\n",
    "- **KPSS Test**: Null hypothesis = stationary (reject if p < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze stationarity\n",
    "analyzer = StationarityAnalyzer()\n",
    "stationarity_result = analyzer.analyze(train_df['y'], name=\"engagement_log\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATIONARITY ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSeries: {stationarity_result['series_name']}\")\n",
    "print(f\"Observations: {stationarity_result['n_observations']}\")\n",
    "\n",
    "print(f\"\\nADF Test:\")\n",
    "print(f\"  Statistic: {stationarity_result['adf_test']['test_statistic']:.4f}\")\n",
    "print(f\"  P-value: {stationarity_result['adf_test']['p_value']:.4f}\")\n",
    "print(f\"  Is Stationary: {stationarity_result['adf_test']['is_stationary']}\")\n",
    "\n",
    "print(f\"\\nKPSS Test:\")\n",
    "print(f\"  Statistic: {stationarity_result['kpss_test']['test_statistic']:.4f}\")\n",
    "print(f\"  P-value: {stationarity_result['kpss_test']['p_value']:.4f}\")\n",
    "print(f\"  Is Stationary: {stationarity_result['kpss_test']['is_stationary']}\")\n",
    "\n",
    "print(f\"\\nRecommendation: {stationarity_result['recommendation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing analysis\n",
    "diff_result = analyzer.differencing_analysis(train_df['y'], max_d=2)\n",
    "print(f\"\\nRecommended differencing order: d={diff_result['recommended_d']}\")\n",
    "\n",
    "# Save stationarity report\n",
    "analyzer.save_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Individual Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Prophet\n",
    "print(\"Training Prophet model...\")\n",
    "prophet_forecaster = ProphetForecaster(\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True\n",
    ")\n",
    "prophet_forecaster.fit(train_prophet)\n",
    "print(\"Prophet training complete!\")\n",
    "\n",
    "# Predict on test set\n",
    "prophet_preds = prophet_forecaster.predict_test(test_prophet)\n",
    "print(f\"\\nProphet predictions: {len(prophet_preds)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet evaluation\n",
    "prophet_eval = prophet_forecaster.evaluate(test_prophet)\n",
    "print(\"\\nProphet Metrics (Original Scale):\")\n",
    "for metric, value in prophet_eval['metrics_original_scale'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SARIMA with auto order selection\n",
    "print(\"Training SARIMA model (auto-selecting order)...\")\n",
    "sarima_forecaster = SARIMAForecaster()\n",
    "\n",
    "# Auto-select order\n",
    "order, seasonal_order = sarima_forecaster.auto_select_order(\n",
    "    train_df['y'],\n",
    "    seasonal=True,\n",
    "    m=7  # Weekly seasonality\n",
    ")\n",
    "print(f\"Selected order: {order}\")\n",
    "print(f\"Selected seasonal order: {seasonal_order}\")\n",
    "\n",
    "# Fit model\n",
    "sarima_forecaster.fit(train_df['y'])\n",
    "print(\"SARIMA training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA diagnostics\n",
    "diagnostics = sarima_forecaster.get_diagnostics()\n",
    "print(f\"\\nSARIMA Diagnostics:\")\n",
    "print(f\"  Order: {diagnostics['order']}\")\n",
    "print(f\"  Seasonal Order: {diagnostics['seasonal_order']}\")\n",
    "print(f\"  AIC: {diagnostics['aic']:.2f}\")\n",
    "print(f\"  BIC: {diagnostics['bic']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA predictions\n",
    "sarima_preds = sarima_forecaster.predict(steps=len(test_df))\n",
    "print(f\"SARIMA predictions: {len(sarima_preds)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LSTM Model\n",
    "\n",
    "**Note:** With only 292 training samples, LSTM may underperform compared to traditional models. This is expected behavior for deep learning with limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "print(\"Training LSTM model...\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Note: 292 samples is borderline for LSTM. Performance may vary.\")\n",
    "\n",
    "lstm_forecaster = LSTMForecaster(\n",
    "    n_units=50,\n",
    "    n_layers=2,\n",
    "    window_size=30,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    dropout=0.2,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "# Select features for LSTM\n",
    "lstm_features = [\n",
    "    'y_lag_1', 'y_lag_7', 'y_lag_14',\n",
    "    'y_rolling_mean_7', 'y_rolling_std_7',\n",
    "    'day_of_week', 'is_weekend'\n",
    "]\n",
    "\n",
    "# Filter to available features\n",
    "lstm_features = [f for f in lstm_features if f in train_df.columns]\n",
    "print(f\"Using features: {lstm_features}\")\n",
    "\n",
    "lstm_forecaster.fit(train_df, feature_cols=lstm_features, verbose=1)\n",
    "print(\"\\nLSTM training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM predictions\n",
    "lstm_preds = lstm_forecaster.predict_test(train_df, test_df)\n",
    "print(f\"LSTM predictions: {len(lstm_preds)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LSTM training history\n",
    "if lstm_forecaster.history:\n",
    "    visualizer = Visualizer()\n",
    "    fig = visualizer.plot_training_history(lstm_forecaster.history)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Forecasting\n",
    "\n",
    "Combining predictions with weights:\n",
    "- Prophet: 40%\n",
    "- SARIMA: 35%\n",
    "- LSTM: 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble forecaster\n",
    "ensemble = EnsembleForecaster(\n",
    "    weights={\n",
    "        'prophet': 0.40,\n",
    "        'sarima': 0.35,\n",
    "        'lstm': 0.25\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add pre-trained models\n",
    "ensemble.add_model('prophet', prophet_forecaster)\n",
    "ensemble.add_model('sarima', sarima_forecaster)\n",
    "ensemble.add_model('lstm', lstm_forecaster)\n",
    "\n",
    "print(\"Ensemble created with models:\", list(ensemble.models.keys()))\n",
    "print(\"Weights:\", ensemble.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ensemble predictions\n",
    "ensemble_preds = ensemble.predict(test_df, train_df=train_df)\n",
    "print(f\"\\nEnsemble predictions: {len(ensemble_preds)} rows\")\n",
    "ensemble_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Metrics\n",
    "\n",
    "**Target Thresholds:**\n",
    "- MAPE < 15%\n",
    "- RMSE < 15% of mean\n",
    "- R² > 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble\n",
    "metrics = ForecastMetrics()\n",
    "\n",
    "# Get predictions and actuals (log scale)\n",
    "y_true_log = test_df['y'].values\n",
    "y_pred_log = ensemble_preds['ensemble'].values\n",
    "\n",
    "# Full evaluation\n",
    "results = metrics.evaluate(y_true_log, y_pred_log, include_log_metrics=True)\n",
    "\n",
    "# Print report\n",
    "metrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "predictions_dict = {\n",
    "    'Prophet': prophet_preds['yhat'].values,\n",
    "    'SARIMA': sarima_preds['forecast'].values,\n",
    "    'LSTM': lstm_preds['yhat'].values,\n",
    "    'Ensemble': ensemble_preds['ensemble'].values\n",
    "}\n",
    "\n",
    "comparison = metrics.compare_models(y_true_log, predictions_dict)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "metrics.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = Visualizer(figsize=(14, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "fig = visualizer.plot_predictions(\n",
    "    dates=test_df['ds'],\n",
    "    y_true=test_df['y_raw'],  # Original scale\n",
    "    predictions={\n",
    "        'Prophet': np.expm1(prophet_preds['yhat'].values),\n",
    "        'SARIMA': np.expm1(sarima_preds['forecast'].values),\n",
    "        'LSTM': np.expm1(lstm_preds['yhat'].values),\n",
    "        'Ensemble': np.expm1(ensemble_preds['ensemble'].values)\n",
    "    },\n",
    "    title=\"Forecast vs Actual (Original Scale)\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals for ensemble\n",
    "y_pred_original = np.expm1(ensemble_preds['ensemble'].values)\n",
    "y_true_original = test_df['y_raw'].values\n",
    "\n",
    "fig = visualizer.plot_residuals(y_true_original, y_pred_original, model_name=\"Ensemble\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison metrics\n",
    "fig = visualizer.plot_model_comparison(comparison)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dashboard\n",
    "fig = visualizer.create_dashboard(\n",
    "    results=results,\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    predictions_df=ensemble_preds\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save dashboard\n",
    "visualizer.save_figure(fig, Config.PROCESSED_DATA_DIR / \"forecast_dashboard.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2 FORECASTING - FINAL RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nData Summary:\")\n",
    "print(f\"  Training samples: {len(train_df)}\")\n",
    "print(f\"  Test samples: {len(test_df)}\")\n",
    "print(f\"  Features used: {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\nModel Performance (on test set):\")\n",
    "print(comparison[['model', 'mape', 'rmse_pct', 'r2', 'all_pass']].to_string(index=False))\n",
    "\n",
    "print(\"\\nTarget Thresholds:\")\n",
    "print(f\"  MAPE < 15%: {results['pass_fail']['mape']}\")\n",
    "print(f\"  RMSE < 15% of mean: {results['pass_fail']['rmse_pct']}\")\n",
    "print(f\"  R² > 0.70: {results['pass_fail']['r2']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if results['all_passed']:\n",
    "    print(\"ALL TARGETS ACHIEVED!\")\n",
    "else:\n",
    "    print(\"SOME TARGETS NOT MET - Review individual model performance\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble results\n",
    "import json\n",
    "\n",
    "ensemble_results = {\n",
    "    'models': list(ensemble.models.keys()),\n",
    "    'weights': ensemble.weights,\n",
    "    'ensemble_metrics': results['metrics_original_scale'],\n",
    "    'pass_fail': results['pass_fail'],\n",
    "    'all_passed': results['all_passed'],\n",
    "    'model_comparison': comparison.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "with open(Config.PROCESSED_DATA_DIR / \"ensemble_results.json\", 'w') as f:\n",
    "    json.dump(ensemble_results, f, indent=2, default=str)\n",
    "\n",
    "print(\"Results saved to data/processed/ensemble_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes on Performance\n",
    "\n",
    "### LSTM with Limited Data\n",
    "With only 292 training samples, LSTM may underperform compared to traditional statistical models (Prophet, SARIMA). This is expected behavior - deep learning models typically require thousands of samples to outperform simpler methods.\n",
    "\n",
    "If LSTM significantly underperforms:\n",
    "> \"Traditional statistical models outperformed deep learning due to limited dataset size (292 samples). For production deployment with more data, LSTM weights could be increased.\"\n",
    "\n",
    "### Ensemble Weights\n",
    "Current weights (Prophet: 40%, SARIMA: 35%, LSTM: 25%) can be optimized using `ensemble.optimize_weights()` with validation data.\n",
    "\n",
    "### Log Scale\n",
    "All predictions are made on log-scale (`y = log1p(engagement)`) and inverse-transformed for evaluation. This normalizes the 1030x scale difference between datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
